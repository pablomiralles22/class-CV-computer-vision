{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyObEjEvu7awOhjLCQh8KF/C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pablomiralles22/class-CV-computer-vision/blob/main/YOLO_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akFfxiHT7Jua"
      },
      "outputs": [],
      "source": [
        "!pip install transformers torch torchvision matplotlib pillow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "from transformers import YolosImageProcessor, YolosForObjectDetection\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "QWdTskB19ery"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üñºÔ∏è **Load Example Image from URL**"
      ],
      "metadata": {
        "id": "zrB3P7ktbpTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
        "image = Image.open(requests.get(url, stream=True).raw)"
      ],
      "metadata": {
        "id": "RIxCaMlhbcpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ü§ñ **Load Pretrained YOLOS Model**"
      ],
      "metadata": {
        "id": "qdYE57Cpb3fe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\n",
        "image_processor = YolosImageProcessor.from_pretrained(\"hustvl/yolos-tiny\")"
      ],
      "metadata": {
        "id": "EEvh-QDLbxxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* **`YolosForObjectDetection.from_pretrained(...)`**:\n",
        "\n",
        "  * Loads the **YOLOS-tiny** object detection model from Hugging Face's `transformers` library.\n",
        "  * `YOLOS` = \"You Only Look One-level Series\", a Vision Transformer-based (ViT) object detector.\n",
        "\n",
        "* **`YolosImageProcessor.from_pretrained(...)`**:\n",
        "\n",
        "  * Loads the corresponding **image preprocessor** (handles resizing, normalization, etc.) required to prepare inputs for the model.\n",
        "\n",
        "\n",
        "`\"hustvl/yolos-tiny\"` is a small, efficient YOLOS variant fine-tuned on COCO."
      ],
      "metadata": {
        "id": "0Oj6t9zjcCxF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üéØ **Run YOLOS Object Detection on an Image**"
      ],
      "metadata": {
        "id": "NZXLi4_QcUdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = image_processor(images=image, return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "\n",
        "# model predicts bounding boxes and corresponding COCO classes\n",
        "logits = outputs.logits\n",
        "bboxes = outputs.pred_boxes\n",
        "\n",
        "# print results\n",
        "target_sizes = torch.tensor([image.size[::-1]])\n",
        "results = image_processor.post_process_object_detection(outputs, threshold=0.9, target_sizes=target_sizes)[0]\n",
        "for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
        "    box = [round(i, 2) for i in box.tolist()]\n",
        "    print(\n",
        "        f\"Detected {model.config.id2label[label.item()]} with confidence \"\n",
        "        f\"{round(score.item(), 3)} at location {box}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "ESlcJSYW7l8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "inputs = image_processor(images=image, return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "```\n",
        "\n",
        "* **Preprocesses** the PIL image into PyTorch tensors.\n",
        "* **Feeds** the image into the YOLOS model to get predictions.\n",
        "\n",
        "```python\n",
        "logits = outputs.logits\n",
        "bboxes = outputs.pred_boxes\n",
        "```\n",
        "\n",
        "* **`logits`**: Raw class scores per object (not directly used here).\n",
        "* **`pred_boxes`**: Predicted bounding boxes in normalized format.\n",
        "\n",
        "\n",
        "üì¶ **Post-process Predictions**\n",
        "\n",
        "```python\n",
        "target_sizes = torch.tensor([image.size[::-1]])\n",
        "results = image_processor.post_process_object_detection(\n",
        "    outputs, threshold=0.9, target_sizes=target_sizes\n",
        ")[0]\n",
        "```\n",
        "\n",
        "* Converts normalized predictions to absolute pixel coordinates.\n",
        "* Filters detections with **confidence ‚â• 0.9**.\n",
        "* `target_sizes`: Image height √ó width (used to scale boxes).\n",
        "\n",
        "\n",
        "üñ®Ô∏è **Print Detected Objects**\n",
        "\n",
        "```python\n",
        "for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
        "    box = [round(i, 2) for i in box.tolist()]\n",
        "    print(\n",
        "        f\"Detected {model.config.id2label[label.item()]} with confidence \"\n",
        "        f\"{round(score.item(), 3)} at location {box}\"\n",
        "    )\n",
        "```\n",
        "\n",
        "* Iterates over high-confidence detections.\n",
        "* Maps label IDs to COCO class names.\n",
        "* Prints object name, confidence score, and bounding box coordinates.\n"
      ],
      "metadata": {
        "id": "7fQqjEAccWoL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üåÅ **Plot the results**"
      ],
      "metadata": {
        "id": "-iW73Qg0cK64"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the image\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.imshow(image)\n",
        "ax = plt.gca()\n",
        "\n",
        "# Add each bounding box and label\n",
        "for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
        "    xmin, ymin, xmax, ymax = box.tolist()\n",
        "    width, height = xmax - xmin, ymax - ymin\n",
        "    rect = patches.Rectangle(\n",
        "        (xmin, ymin), width, height,\n",
        "        linewidth=2, edgecolor=\"red\", facecolor=\"none\"\n",
        "    )\n",
        "    ax.add_patch(rect)\n",
        "    label_text = f\"{model.config.id2label[label.item()]}: {score.item():.2f}\"\n",
        "    ax.text(xmin, ymin - 5, label_text, color=\"white\",\n",
        "            fontsize=12, bbox=dict(facecolor=\"red\", alpha=0.5))\n",
        "\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-CD-5WPo9W2A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}